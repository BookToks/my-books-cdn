In this case, the terms inside the brackets are all parts of a formal series.

So, instead of writing “(– – – – –T)(ξ,....)”, I just write “N(ξ)”.

N(ξ) means the opposite of all the values of the variable ξ.

It's pretty easy to show how you can build propositions using this operation and how you can't build them with it. So, it must be possible to explain this clearly.

If ξ has only one value, then N(ξ) means “not p”. If it has two values, then N(ξ) means “not p and not q” (neither p nor q).

How can the big logic system, which reflects the world, use things like these special tricks? It can only do this because all these parts are connected in a super-fine network, like a big mirror.

“∼p” is true if “p” is false. So in the true statement “∼p”, the statement “p” is false. But how can the symbol “∼” make it match reality?

The thing that denies in “∼p” isn’t just the symbol “∼”, but what all these signs that deny p have in common.

That’s why there’s a common rule for building things like “∼p”, “∼∼∼p”, “∼p∨∼p”, “∼p.∼p”, and so on forever. And what they all share is the idea of denial.

We could say: What all symbols that say both p and q have in common is “p and q” (p.q). What all symbols that say either p or q have in common is “p or q” (p∨q).

Similarly, two propositions are opposite when they don’t share anything. And every proposition has only one negative because there’s only one proposition that is completely outside it.

So even in Russell’s notation, “q : p∨∼p” means the same as “q”, and “p∨∼p” doesn’t say anything.

If a notation is fixed, then it has rules for how to build all the propositions that deny p, all that assert p, all that assert p or q, and so on.
